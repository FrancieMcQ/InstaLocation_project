{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: Instructions on how we generated the training data is at the bottom of the notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#files\n",
    "ground_truth = pd.read_csv(\"/Users/frances/Documents/Tribe/ops/InstaLocation_project/Ground_truths/groundtruth.csv\")\n",
    "f2_lr_filtered = pd.read_csv(\"/Users/frances/Documents/Tribe/ops/InstaLocation_project/lr_filtered_for_f2.csv\")\n",
    "all_partial_locs = pd.read_csv(\"/Users/frances/Documents/Tribe/ops/InstaLocation_project/Location_info/all_locations.csv\")\n",
    "#regexs for whole word search\n",
    "airports = pd.read_csv(\"/Users/frances/Documents/Tribe/ops/InstaLocation_project/Location_info/airport_info.csv\")\n",
    "airport_reg = re.compile(r\"\\b({})\\b\".format(\"|\".join(airports['Code'])))\n",
    "country_abvs= pd.read_csv(\"/Users/frances/Documents/Tribe/ops/InstaLocation_project/Location_info/country_abbvs.csv\")\n",
    "country_abv_reg = re.compile(r\"\\b({})\\b\".format('|'.join(str(v) for v in country_abvs[\"Abbreviation\"])))\n",
    "upper_state_abv = pd.read_csv(\"/Users/frances/Documents/Tribe/ops/InstaLocation_project/Location_info/state_abbreviations.csv\")\n",
    "upper_state_abv_reg = re.compile(r\"\\b({})\\b\".format(\"|\".join(upper_state_abv[\"Abbreviation\"])))\n",
    "#set for a quick search for a feature\n",
    "ethnicities = pd.read_csv(\"/Users/frances/Documents/Tribe/ops/InstaLocation_project/Location_info/nationalities.csv\")\n",
    "set_ethnicities = set(ethnicities['Nationality'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** If you want the ground_truth file before we added our manual 1's and 0's, here it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_up_groundtruth = pd.read_csv(\"/Users/frances/Documents/Tribe/ops/InstaLocation_project/Ground_truths/groundtruth_setup.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important: Run all little functions before big one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ease of reading, and clarity, I put the big main function first, but all the little helper functions must be defined before you run the big one so the computer knows what the variables are referencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def everything(text):\n",
    "    \"\"\"The function that consolidates the whole location process in one function. \n",
    "    text: The IG profile description text, as a string.\n",
    "    Returns a numpy array of only 'home' locations- that is, only the loctions in which the \n",
    "    model was more than 50% confident that it was an actual location and more than 50% confident\n",
    "    that it's a home location.\"\"\"\n",
    "    #Step 1: Put text into correct format\n",
    "    text_df = pd.DataFrame(data = np.array([text]),\\\n",
    "                           columns = np.array([\"combined_descriptions\"]))\n",
    "    \n",
    "    #Step 2: Extract Locations\n",
    "    with_locations_df = extract_location(text_df, \"combined_descriptions\")\n",
    "    \n",
    "    #Step 3: Expand Dataframe - one location per row\n",
    "    expanded = expand_location_df(with_locations_df)\n",
    "    \n",
    "    #Step 4: Find the Original Location\n",
    "    with_original = make_origin_loc_column(expanded, \"combined_descriptions\", \"matched_location\")\n",
    "    \n",
    "    #Step 5: Extract the whole context\n",
    "    with_context = make_context_column(with_original, \"combined_descriptions\", \"original_location\")\n",
    "    \n",
    "    #Step 6: Extract Before and After Context\n",
    "    everything_df = make_before_and_after_columns(with_context, \"Context\", \"original_location\")\n",
    "    \n",
    "    #Step 7: Calculate Filter 1 Features\n",
    "    feature_list = return_features_list(everything_df)\n",
    "    cv_word_features = f1_make_cv_word_features(everything_df[\"Context\"])\n",
    "    f1_df_features_lst = df_features(everything_df)\n",
    "    f1_df_features = return_df_features(f1_df_features_lst, everything_df) #into proper format\n",
    "    all_features = combine_feature_lists(feature_list, cv_word_features, f1_df_features)\n",
    "    top_50 = all_features[:,f1_inx]\n",
    "    f1_features = np.array(list(top_50)) #formatting\n",
    "    \n",
    "    #Step 8: Feed to Filter 1 Model\n",
    "    lr_prediction = f1_lr.predict_proba(f1_features)\n",
    "    lr_loc_probs = [prob[1] for prob in lr_prediction]\n",
    "    everything_df[\"f1_prob\"] = lr_loc_probs\n",
    "    everything_df[\"f1_binary\"] = (everything_df[\"f1_prob\"] > 0.5).astype(int)\n",
    "    \n",
    "    #Step 9: Filter Out Non-Locations\n",
    "    actual_locations = everything_df.where(everything_df[\"f1_prob\"] > 0.50).dropna()\n",
    "    \n",
    "    #Step 10: Calculate Filter 2 Features\n",
    "    f2_feature_list = f2_return_features_list(actual_locations)\n",
    "    f2_cv_word_features = f2_make_cv_word_features(actual_locations[\"Context\"])\n",
    "    f2_df_features_lst = df_features(actual_locations)\n",
    "    f2_df_features = return_df_features(f2_df_features_lst, actual_locations)\n",
    "    f2_all_features = combine_feature_lists(f2_feature_list, f2_cv_word_features, f2_df_features)\n",
    "    f2_top_50 = f2_all_features[:,f2_inx] \n",
    "    f2_features = np.array(list(f2_top_50)) #formatting\n",
    "    \n",
    "    #Step 11: Feed to Filter 2 Model\n",
    "    f2_lr_prediction = f2_lr.predict_proba(f2_features)\n",
    "    f2_lr_loc_probs = [prob[1] for prob in f2_lr_prediction]\n",
    "    actual_locations[\"f2_prob\"] = f2_lr_loc_probs\n",
    "    actual_locations[\"f2_binary\"] = (actual_locations[\"f1_prob\"] > 0.5).astype(int)\n",
    "    \n",
    "    #Step 12: Find only Home Locations and Return \n",
    "    only_home_df = actual_locations.where(actual_locations[\"f2_prob\"] > 0.50)\n",
    "    homes_array = only_home_df[\"original_location\"].as_matrix()\n",
    "    return homes_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Miami'], dtype=object)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example\n",
    "text = ''';Fashion Producer/Editor - Will bring you the lastest in Fashion, Style and Beauty. NYC, Houston, Chicago and Miami - Follow me on Twitter: @LuisSoto2u website: '''\n",
    "everything(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Everything Below this Cell before running above cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Features for models\n",
    "f1_ground_truth_features = np.load(\"/Users/frances/Documents/Tribe/ops/InstaLocation_project/ground_truth_features.npy\")\n",
    "f1_ground_truth_response = ground_truth[\"is_location?\"]\n",
    "f2_ground_truth_features = np.load(\"/Users/frances/Documents/Tribe/ops/InstaLocation_project/f2_lr_features.npy\")\n",
    "f2_ground_truth_response = f2_lr_filtered[\"is_home?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter 1 Model\n",
    "f1_lr = LogisticRegression()\n",
    "f1_lr.fit(f1_ground_truth_features, f1_ground_truth_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filter 2 Model\n",
    "f2_lr = LogisticRegression()\n",
    "f2_lr.fit(f2_ground_truth_features, f2_ground_truth_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find top 50 features\n",
    "f1_rf = RandomForestClassifier()\n",
    "f1_rf.fit(f1_ground_truth_features, f1_ground_truth_response)\n",
    "f1_importance_arr = f1_rf.feature_importances_\n",
    "f1_inx = (-f1_importance_arr).argsort()[:50]\n",
    "f2_rf = RandomForestClassifier()\n",
    "f2_rf.fit(f2_ground_truth_features, f2_ground_truth_response)\n",
    "f2_importance_arr = f2_rf.feature_importances_\n",
    "f2_inx = (-f2_importance_arr).argsort()[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 (Extract Locations) Little Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_location(df, map_column):\n",
    "    \"\"\"Adds a column to original df where each cell contains a list of locations found \n",
    "    in the text of that row\n",
    "    df: Panda dataframe containing the instagram profile description, as well as ambassador ids.\n",
    "    map_column: The column name (as a string) that contains the profile text.\"\"\"\n",
    "    \n",
    "    def whole_compiled_search(compiled_reg, text):\n",
    "        \"\"\"Searches for whole-word search terms and outputs any that is found in the text.\n",
    "        compiled_reg: A compiled regex search object containing either airport codes or \n",
    "        US state abbreviations.\n",
    "        text: the text of ONE Instagram profile description.\"\"\"\n",
    "        text = re.sub(r'[^\\w\\s]','',text)\n",
    "        if isinstance(text, float): #catches NaNs\n",
    "            return []\n",
    "        return compiled_reg.findall(text) #return words that match\n",
    "    \n",
    "    def partial_everything_search(text):\n",
    "        \"\"\"Searches for partial-word seach terms in the text (such as cities, countries,\n",
    "        nationalities, or flag emojis), and returns them. \n",
    "        text: the text of ONE Instagram profile description\"\"\"\n",
    "        def partial_helper(searchterm):\n",
    "            nonlocal partial_matches\n",
    "            if searchterm.lower() in text.lower():\n",
    "                partial_matches.append(searchterm)\n",
    "        partial_matches = []\n",
    "        if isinstance(text, float): #catches NaNs\n",
    "            return []\n",
    "        else:\n",
    "            all_partial_locs[\"Name\"].map(partial_helper)# map search function to a list of every location\n",
    "            return partial_matches\n",
    "        \n",
    "    partial_terms_ser = df[map_column].map(partial_everything_search)\\\n",
    "                                      .rename(\"partial\") #find partial search terms\n",
    "    airport_ser = df[map_column].map(lambda text:whole_compiled_search(airport_reg, text))\\\n",
    "                                .rename(\"airport\") #find airports\n",
    "    state_abvs_ser = df[map_column].map(lambda text:whole_compiled_search(upper_state_abv_reg, text))\\\n",
    "                                   .rename(\"state_abvs\") #find state abbv\n",
    "    country_abvs_ser = df[map_column].map(lambda text:whole_compiled_search(country_abv_reg, text))\\\n",
    "                                   .rename(\"country_abvs\") #find country abbv\n",
    "    \n",
    "    placeholder = partial_terms_ser.add(airport_ser) \n",
    "    second_placeholder = placeholder.add(country_abvs_ser)\n",
    "    all_together_ser = second_placeholder.add(state_abvs_ser) #combine all locations together in one series\n",
    "    locations = all_together_ser.rename(\"matched_location\").to_frame() #make into df\n",
    "    \n",
    "    def replace_with_none(locations_list): # consistency\n",
    "        if locations_list == []:\n",
    "            locations_list = [\"None\"]\n",
    "        return locations_list\n",
    "    \n",
    "    locations[\"matched_location\"] = locations[\"matched_location\"].map(replace_with_none)\n",
    "    \n",
    "    #join \"location\" column to original df and return \n",
    "    return df.join(locations).fillna(value = \"None\") #fillna so next function will work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 (Expand) Little Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#link for solution: https://stackoverflow.com/questions/26068021/iterate-over-rows-and-expand-pandas-dataframe\n",
    "def expand_location_df(df):\n",
    "    '''Dataframe must have NaNs filled with some value, preferably \"None\".\n",
    "    df: the dataframe returned by the df_with_location function. \n",
    "    Must have a \"location\" column that contains a list of locations, or list of \"None\".'''\n",
    "    \n",
    "    #expands the locations and profile description text\n",
    "    def expand_descriptions(row):\n",
    "        locations = row['matched_location'] if isinstance(row['matched_location'], list) else [row['matched_location']]\n",
    "        s = pd.Series(row['combined_descriptions'], index=list(set(locations)))\n",
    "        return s\n",
    "    \n",
    "    #formatting\n",
    "    df_expand_text = df.apply(expand_descriptions, axis=1).stack()\n",
    "    df_expand_text = df_expand_text.to_frame().reset_index(level=1, drop=False)\n",
    "    df_expand_text.columns = ['matched_location', 'combined_descriptions']\n",
    "    df_expand_text.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df_expand_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code to add to bottom of expand df function if you want to track ambassador name as well**\n",
    "#expands the ambassador names\n",
    "\n",
    "    def expand_name(row):\n",
    "        locations = row['matched_location'] if isinstance(row['matched_location'], list) else [row['matched_location']]\n",
    "        s = pd.Series(row['ambassador_name'], index=list(set(locations)))\n",
    "        return s\n",
    "    \n",
    "    #formatting\n",
    "    df_expand_name = df.apply(expand_name, axis=1).stack()\n",
    "    df_expand_name = df_expand_name.to_frame().reset_index(level=1, drop=False)\n",
    "    df_expand_name.columns = ['matched_location', 'ambassador_name']\n",
    "    df_expand_name.reset_index(drop=True, inplace=True)\n",
    "    df_expand_name = df_expand_name.drop('matched_location', axis = 1) #drop extra location column to join\n",
    "    df_expand_name = df_expand_name[\"ambassador_name\"]\n",
    "    \n",
    "    #join ids with locations and descriptions\n",
    "    with_descrip_and_ids = df_expand_text.join(df_expand_ids)\n",
    "\n",
    "    #join ids with locations and descriptions\n",
    "    with_name = with_descrip_and_ids.join(df_expand_name)\n",
    "    return with_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 (Find original) Little Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_original_location(location, text):\n",
    "    \"\"\"Given a location string, returns how the location is formatted in the text.\n",
    "    For example, if text is 'I live in PARIsc', and location is 'paris', returns 'PARIsc'.\n",
    "    Note: the location passed in must have been found in the text passed in!\n",
    "    text: The text (as a string) of a profile description\n",
    "    location: a location that Step 2 function found in the above text.\"\"\"\n",
    "    \n",
    "    if location == \"None\": #there's no original location\n",
    "        return location\n",
    "    \n",
    "    elif len(location) > 3: #partial word search\n",
    "        word_object = re.search(r'\\w*(?:{search_term})\\w*'.format(search_term=\\\n",
    "                    re.escape(location)), text, flags = re.IGNORECASE)\n",
    "        return word_object[0]\n",
    "    \n",
    "    else: #whole word search-for things like airport codes and state abbvs\n",
    "        return location #because the location function only finds it if it exactly matches our list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_origin_loc_column(df, text_column, location_column):\n",
    "    \"\"\"Must use exapanded row df with location column. Returns original df with an\n",
    "    'Original Location' column, that contains how the 'matched' location is formatted in the text.\n",
    "    df: dataframe, with expanded rows, that contains the location column from Step 2.\n",
    "    text_column: The name (as as string) of the column containing the profile descriptions\n",
    "    location_column: The name (as a string) of the column containing the location found in text.\"\"\"\n",
    "    original_loc = []\n",
    "    for index, row in df.iterrows(): #iterate through rows of df\n",
    "        text = row[text_column]\n",
    "        loc = row[location_column] #find paramters\n",
    "        original_loc.append(find_original_location(loc, text)) #append original loc to list\n",
    "    original_df = pd.Series(original_loc)\\\n",
    "                    .rename(\"original_location\")\\\n",
    "                    .to_frame() #make list to series to dataframe to add as a column\n",
    "    return df.join(original_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 (extract context) Little Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_context(keyword, text):\n",
    "    \"\"\"Returns a string containing the keyword (a location), and the 5 words before and \n",
    "    after the keyword in the text. If keyword is \"None\" (ie no location was found), it returns \"None\"\n",
    "    Keyword: a location string or 'None'.\n",
    "    text: One Instagram profile description text, as a string\"\"\"\n",
    "    if keyword is not \"None\":\n",
    "        before_keyword, keyword_list, after_keyword = text.partition(keyword)\n",
    "        #below turns strings to lists of words so we can count indices\n",
    "        before_keyword, keyword_list, after_keyword = before_keyword.split(), keyword_list.split(), after_keyword.split()\n",
    "        if len(before_keyword) >= 5: \n",
    "            left_words_list = before_keyword[len(before_keyword)-5:] #take only 5 words\n",
    "        else:\n",
    "            left_words_list = before_keyword #since less than 5 words, take all \n",
    "        if len(after_keyword) >= 5:\n",
    "            right_words_list = after_keyword[:5] #take only 5 words\n",
    "        else: \n",
    "            right_words_list = after_keyword #since less than 5 words, takes all\n",
    "        left = \" \".join(left_words_list) #turn lists back to strings\n",
    "        right = \" \".join(right_words_list)\n",
    "        return left + \" \" + keyword + \" \" + right #stick everything together\n",
    "    return keyword #if the location was None, return None (there is no context if a location not found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_context_column(df, text_column, orig_location_column):\n",
    "    \"\"\"Must use with the expanded location df (ie such that there is one location per row).\n",
    "    Returns the original df, with a new 'Context' column, that contains the context of each \n",
    "    location found (one context per row)\n",
    "    df: expanded dataframe containing a text column, and the location column from Step 2.\n",
    "    text_column: The name (as a string) of the column containing the profile descriptions.\n",
    "    location_column: The name (as a string) of the column containing location found in text\"\"\"\n",
    "    context_list = []\n",
    "    for index, row in df.iterrows(): #note: internet says itertuples is faster but gives us errors\n",
    "        text = row[text_column]\n",
    "        loc = row[orig_location_column]\n",
    "        context = extract_context(loc, text) #pass parameters to other function\n",
    "        context_list.append(context) #append context to list\n",
    "    context_df = pd.Series(context_list)\\\n",
    "                    .rename(\"Context\")\\\n",
    "                    .to_frame() #convert to dataframe for next step\n",
    "    return df.join(context_df) #add contex column to df."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 (before and After context) Little Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def before_context(context, original_location):\n",
    "    \"\"\"Context: String from 'Context' column of the dataframe. Is the 5 words before and after\n",
    "    location found\n",
    "    original_location: The string from the 'original_location' column- the location found.\n",
    "    Returns the 5 words before the location\"\"\"\n",
    "    before, loc, after = context.partition(original_location)\n",
    "    return before\n",
    "\n",
    "def after_context(context, original_location):\n",
    "    \"\"\"Context: String from 'Context' column of the dataframe. Is the 5 words before and after\n",
    "    location found\n",
    "    original_location: The string from the 'original_location' column- the location found.\n",
    "    Returns the 5 words after the location\"\"\"\n",
    "    before, loc, after = context.partition(original_location)\n",
    "    return after\n",
    "\n",
    "def make_before_and_after_columns(df, context_column, orig_location_column):\n",
    "    \"\"\"Must use with the expanded location df (ie such that there is one location per row).\n",
    "    Returns the original df, with two columns added- a before context column, and an after \n",
    "    context column\n",
    "    df: expanded dataframe containing a text column, and the location column from Step 2.\n",
    "    text_column: The name (as a string) of the column containing the profile descriptions.\n",
    "    location_column: The name (as a string) of the column containing location found in text\"\"\"\n",
    "    before_list = []\n",
    "    after_list = []\n",
    "    for index, row in df.iterrows(): #note: internet says itertuples is faster but gives us errors\n",
    "        contxt = row[context_column]\n",
    "        loc = row[orig_location_column]\n",
    "        before = before_context(contxt, loc) #pass parameters to other function\n",
    "        before_list.append(before) #append before_context to list\n",
    "        after = after_context(contxt, loc)\n",
    "        after_list.append(after)\n",
    "    before_context_df = pd.Series(before_list)\\\n",
    "                    .rename(\"before_loc\")\\\n",
    "                    .to_frame()\n",
    "    after_context_df = pd.Series(after_list)\\\n",
    "                    .rename(\"after_loc\")\\\n",
    "                    .to_frame()\n",
    "    split_context_df = before_context_df.join(after_context_df)\n",
    "    return df.join(split_context_df) #add contex column to df."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7 (Filter 1 Features) Little Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def return_feature_array_for_row(row):\n",
    "    \"\"\"Returns a array of features for that one row. \n",
    "    These features only need the row to be calculated. \n",
    "    Different for each location candidate\n",
    "    Will be applied to the df returned by ground truth functions\"\"\"\n",
    "    #setup paramters/variables\n",
    "    keyword = row[\"matched_location\"]\n",
    "    text = row['combined_descriptions']\n",
    "    original = row['original_location']\n",
    "    context = row['Context']\n",
    "    common_abvs = [\"CA\", \"UK\", \"US\", \"NY\", \"LA\"]\n",
    "    uncommon_abvs = [\"PR\", \"IN\", \"ME\", \"SO\", \"DM\", \"IT\", \"SC\", \"BY\", \"AT\", \"OR\", \"TV\", \"ET\", \"TO\"]\n",
    "    pushpin = int(\"📍\" in context)\n",
    "    live = fast_search(\"live\", context)\n",
    "    based = fast_search(\"based\", context)\n",
    "    is_from = fast_search(\"from\", context) #filter 2\n",
    "    airplane = int(\"✈️\" in context) #filter 2\n",
    "    is_next = fast_search(\"next\", context)# filter 2\n",
    "    born = fast_search(\"born\", context)#filter 2\n",
    "    currently = fast_search(\"currently\", context)\n",
    "    #features\n",
    "    len_key_matches_orig = compare_len_of_locs(keyword, original)\n",
    "    punc_matches = int(keyword == original)\n",
    "    len_keyword = len(keyword)\n",
    "    num_spaces_keyword = len(keyword.split()) - 1 \n",
    "    good_indicators = surrounding_good(pushpin, live, based, currently)\n",
    "    bad_indicators = surrounding_bad(is_from, airplane, is_next, born)\n",
    "    emoji_or_not = is_emoji(keyword)\n",
    "    is_common = fast_search(keyword, common_abvs)\n",
    "    is_uncommon = fast_search(keyword, uncommon_abvs)\n",
    "    is_whole = is_whole_word(text, keyword) #trying to catch word-within-a-word problems\n",
    "    num_upper_words = count_upper_words(context, original)\n",
    "    model = int(\"model\" in context)\n",
    "    return [len_key_matches_orig, punc_matches, len_keyword, num_spaces_keyword, \\\n",
    "            good_indicators, bad_indicators, emoji_or_not,is_common, is_uncommon, \\\n",
    "            is_whole, num_upper_words, model] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Feature to add to filter one features funtion above if you want to track ambassador name**\n",
    "same_as_name = int(original in row[\"ambassador_name\"])\n",
    "**Add above code after \"model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def df_features(df):\n",
    "    \"\"\"Calculates two features that require the whole dataframe to calculate.\n",
    "    Will be the same for every location found in a single piece of text.\n",
    "    Df: The dataframe returned after step 6\"\"\"\n",
    "    num_other_locations = len(df)\n",
    "    num_flag_emojis = fast_emoji_search(df)\n",
    "    return [num_other_locations, num_flag_emojis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def return_features_list(df):\n",
    "    \"\"\"Applies the return_feature_array_for_row and returns a array of lists, so one list of\n",
    "    features for each row\"\"\"\n",
    "    return df.apply(return_feature_array_for_row, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def return_df_features(df_features_lst, df):\n",
    "    \"\"\"Takes the list of 2 features returned by the df_features function, and dupilcates the\n",
    "    values for each location found in the text (since these features are same for each location)\n",
    "    Puts them into proper format to be combined with the other 2 sets of features calculated\"\"\"\n",
    "    new_long_lst = []\n",
    "    for i in range(len(df)):\n",
    "        new_long_lst.append(df_features_lst)\n",
    "    return np.array(new_long_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_feature_lists(our_features_ser, cv_sparse_matrix, df_features_arr):\n",
    "    \"\"\"Combines 3 sets of feature arrays into one array, so that each location candidate has\n",
    "    all its features in one place.\n",
    "    our_features_ser: The features returned by return_feature_array_for_row, ie the features\n",
    "    we created that only need to access one row\n",
    "    cv_sparse_matrix: The sparse matrix of features returned by a count vectorizer (more features)\n",
    "    df_features_arr: The features returned by return_df_features (ie the features we created that\n",
    "    need to access the whole df)\n",
    "    Returns: a numpy array that will be passed to models.\"\"\"\n",
    "    \n",
    "    #reformat our features list into a df with each feature as its own column\n",
    "    #link for source: https://chrisalbon.com/python/pandas_expand_cells_containing_lists.html\n",
    "    our_features_df = our_features_ser.rename('features').to_frame()\n",
    "    our_features = our_features_df[\"features\"].apply(pd.Series).as_matrix()\n",
    "    \n",
    "    #reformat sparse matrix:\n",
    "    cv_sparse_to_array = np.array(cv_sparse_matrix.todense())\n",
    "    \n",
    "    #return array of all features \n",
    "    return np.hstack((our_features, cv_sparse_to_array, df_features_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fast_search(keyword, sequence):\n",
    "    \"\"\"A faster way to see if our keywords are in the context.\n",
    "    keyword: A string of indicator word, like 'live' or 'from'. \n",
    "    Keyword CANNOT be an emoji-those don't work with sets. Use 'in' method seperately \n",
    "    sequence: Either a string (like if searching the context), or a list (like if looking\n",
    "    at abbreviations)\"\"\"\n",
    "    if isinstance(sequence, str):\n",
    "        keyword, sequence = re.sub(r'[^\\w\\s]','',keyword), re.sub(r'[^\\w\\s]','', sequence)\n",
    "        keyword, sequence = keyword.lower(), sequence.lower()\n",
    "        keyword_set = set([keyword])\n",
    "        sequence_set = set(sequence.split())\n",
    "        return np.count_nonzero(keyword_set.intersection(sequence_set))\n",
    "    else: #list - no splitting needed\n",
    "        keyword_set = set([keyword])\n",
    "        sequence_set = set(sequence)\n",
    "        return np.count_nonzero(keyword_set.intersection(sequence_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f1_make_cv_word_features(context):\n",
    "    \"\"\"Creates a Count Vectorizer to create features\n",
    "    Context: The whole (ie 10 word) context from the dataframe's 'Context' column\n",
    "    Returns:  a sparse matrix of features\"\"\"\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    cv = CountVectorizer(\n",
    "        ngram_range = (1, 3)\n",
    "    )\n",
    "    return cv.fit_transform(context)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fast_emoji_search(df):\n",
    "    \"\"\"Calculates a feature for feature lists.\n",
    "    Counts how many of the location canidates are flag emojis\n",
    "    returns an int of count.\"\"\"\n",
    "    count = 0 \n",
    "    for index, row in df.iterrows():\n",
    "        if re.search('[a-zA-Z]', row['matched_location']) == None:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_upper_words(context, original_loc):\n",
    "    \"\"\"Returns the number of all-caps words in the context of a location. Designed to catch \n",
    "    country 'abbreviations' that weren't actually locations but just someone saying a bunch\n",
    "    of capitalized stuff\"\"\"\n",
    "    if len(original_loc) < 3:\n",
    "        word_list = context.split()\n",
    "        upper_list = [word for word in word_list if word.isupper()]\n",
    "        return len(upper_list)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def surrounding_good(f1, f2, f3, f4):\n",
    "    \"\"\"Returns 1 if the keyword is surrounded by one of the good location indicators\n",
    "    0 if not. The 'good' indicators are pushpin, live, and based\"\"\"\n",
    "    if f1 or f2 or f3 or f4== 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def surrounding_bad(f4, f5, f6, f7):\n",
    "    \"\"\"Returns 1 if the keyword is surrounded by one of the bad location indicators, 0 otherwise\n",
    "    Bad indicators are based, airplane, and next, bc these signal non-home locations (for filter 2)\"\"\"\n",
    "    if f4 or f5 or f6 or f7 == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#link: https://stackoverflow.com/questions/5319922/python-check-if-word-is-in-a-string\n",
    "def is_whole_word(text, word):\n",
    "    \"\"\"Checks to see if word is a whole word, with spaces before and after.\n",
    "    Deals with word-within-words problem.\n",
    "    Returns 0 if word is within another word\n",
    "    Returns 1 if word is its own seperate whole word\"\"\"\n",
    "    search_object = re.search(r'\\b({0})\\b'.format(word), text, flags=re.IGNORECASE)\n",
    "    if search_object == None:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_len_of_locs(matched, original):\n",
    "    \"\"\"Returns whether the length of the matched location is the same as length of \n",
    "    the original. Designed to catch things that aren't actually locations. Like 'erie' \n",
    "    coming from 'experiences'\"\"\"\n",
    "    matched, original = re.sub(r'[^\\w\\s]','',matched), re.sub(r'[^\\w\\s]','',original) #remove punc\n",
    "    matched, original = matched.lower(), original.lower()\n",
    "    if matched == original:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_emoji(matched_location):\n",
    "    \"\"\"whether or not the location is an emoji\n",
    "    Returns 1 if emoji, 0 if not\"\"\"\n",
    "    if re.search('[a-zA-Z]', matched_location) == None:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10 (Filter 2 features) Little Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f2_return_feature_array_for_row(row):\n",
    "    \"\"\"Returns a array of features for that one row. \n",
    "    Will be applied to the df returned by ground truth functions\"\"\"\n",
    "    #setup paramters/variables\n",
    "    keyword = row[\"matched_location\"]\n",
    "    text = row['combined_descriptions']\n",
    "    original = row['original_location']\n",
    "    context = row['Context']\n",
    "    before_loc = row[\"before_loc\"]\n",
    "    after_loc = row['after_loc']\n",
    "    #features\n",
    "    pushpin_b4 = int(\"📍\" in before_loc)\n",
    "    live_b4 = fast_search(\"live\", before_loc)\n",
    "    living_b4 = fast_search(\"living\", before_loc)\n",
    "    living_after = fast_search(\"living\", after_loc)\n",
    "    based_b4 = fast_search(\"based\", before_loc)\n",
    "    based_after = fast_search(\"based\", after_loc)\n",
    "    is_from_b4 = fast_search(\"from\", before_loc) #filter 2\n",
    "    airplane_b4 = int(\"✈️\" in before_loc) #filter 2\n",
    "    airplane_after = int(\"✈️\" in after_loc) #filter 2\n",
    "    is_next_b4 = fast_search(\"next\", before_loc)# filter 2\n",
    "    born_b4 = fast_search(\"born\", before_loc)#filter 2\n",
    "    via_b4 = fast_search(\"via\", before_loc) #filter 2\n",
    "    soon_emoji_b4 = int(\"🔜\" in before_loc)#filter 2\n",
    "    currently_b4 = fast_search(\"currently\", before_loc)\n",
    "    located_b4 = fast_search(\"located\", before_loc)\n",
    "    emoji_or_not = is_emoji(keyword)\n",
    "    nationality_or_not = is_ethnicity(keyword)\n",
    "    f1_result = row[\"f1_prob\"]\n",
    "    living_in_after = fast_search(\"living in\", after_loc)\n",
    "    based_in_after = fast_search(\"based in\", after_loc)\n",
    "    in_after = fast_search(\"in\", after_loc)\n",
    "    in_before = fast_search(\"in\", before_loc)\n",
    "    college_context = fast_search(\"college\", context)\n",
    "    soon_context = fast_search(\"soon\", context)\n",
    "    model = int(\"model\" in context)\n",
    "    \n",
    "    return [pushpin_b4, live_b4, living_b4, living_after, based_b4, based_after, is_from_b4, \\\n",
    "            airplane_b4, airplane_after, is_next_b4, born_b4, via_b4, soon_emoji_b4, \\\n",
    "            currently_b4, located_b4, emoji_or_not, nationality_or_not, \\\n",
    "            f1_result, living_in_after, based_in_after, in_after, in_before,\\\n",
    "            college_context, soon_context, model] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f2_return_features_list(df):\n",
    "    \"\"\"Applies the return_feature_array_for_row to each row and returns a array of lists, \n",
    "    so one list of features for each row\"\"\"\n",
    "    ser = df.apply(f2_return_feature_array_for_row, axis = 1)\n",
    "    return ser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_ethnicity(matched_location):\n",
    "    \"\"\"Checks to see if the location in the 'matched_location' column is an ethnicity.\n",
    "    Hopefully helps filter 2 distinguish home locations from origins.\n",
    "    Returns 1 if matched_location is an ethnicity, 0 if not.\"\"\"\n",
    "    return int(matched_location in set_ethnicities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2_make_cv_word_features(context):\n",
    "    \"\"\"Creates a Count Vectorizer to create features\n",
    "    Context: The whole (ie 10 word) context from the dataframe's 'Context' column\n",
    "    Returns:  a sparse matrix of features\"\"\"\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    cv = CountVectorizer(\n",
    "        ngram_range = (1, 3)\n",
    "    )\n",
    "    return cv.fit_transform(context)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How We Generated our Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First, we made several SAPD files (from RAF). In searching for profiles, we sometimes used neutral search terms (such as \"fragrance\" or \":earth_americas:\") -- these search terms did not imply a specific location and thus provided us with a variety of geographic locations. Other times, we used more specific search terms like \"Mali\" or \"Los Angeles\" in order to further analyze the errors that come up with specific locations.\n",
    "* Second, we put all the SAPD scrapes into one file (about 825 unique influencers). \n",
    "* Third, we ran steps 2 through 6 in the \"everything\" function, to create a set up for ground truth\n",
    "     * The file needed to have one location per row, have matched_location, original_location, combined_descriptions, Context, before_loc, after_loc, and ambassador id columns\n",
    "* Fourth, we exported that set up file to Google Sheets, where we deleted the index column (a byproduct of exporting a panda dataframe), and added an \"is_location?\"(filter 1) column and an \"is_home?\" (filter 2) column. \n",
    "* Fifth, we manually went through all 2011 rows and read the profile description, and manually marked whether the candidate was a location (1) or not location (0) and whether it was a home location (1) or not a home location (0).\n",
    "    * This is not necessary for most use of the function, because you want a machine to do it for you. This step is only necessary if you want to create more ground truth training data for the models\n",
    "* Sixth, we took the ground_truth file (with our manual checking), and completed steps 7 through 9 in the \"everything\" model above. (ie filter 1 features and model)\n",
    "     * Since we were testing 3 different models, we created 3 different \"filtered locations\" files (ie we created an svc_filtered, an rf_filtered, and an lr_filtered file, each containing only the candidates where the model was more than 50% confident that it was a location. Since each model had different results, we wanted to preserve that in creating filtered df). However, in the \"everything\" function, we only trained a logistic regression model, since that seemed to be the most accurate in our testing.\n",
    "* Seventh, completed steps 10 through 11 (ie filter 2 features and model) on the filtered datasets(ie only on actual locations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Improvements Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For Proper Names problem:\n",
    "    * Though in our groundtruths we did not keep ambassador names, it will be beneficial to use this information in determining whether a person is mentioning a proper name or a location. Also, adding a list of family words and checking if any of those words appear in the context would help solve the problem of people writing phrases like \"Mommy to Austin.\" Some family words that we have seen are: \n",
    "        * mom; mommy; grandma; children; son; daughter; sons; daughters\n",
    "    * Additionally, since some names are common in posts about lifestyle brands, it could help to hardcode out some specific names (like Elizabeth and James) to not be confused with the corresponding locations (Elizabeth in Indiana)\n",
    "* More Location Indicator words:\n",
    "    * house emoji \"🏠\"\n",
    "    * the word \"home\"\n",
    "* Demonyms: \n",
    "    * Currently, we are using a csv of nationalities instead of demonyms. Expanding this to include all types of demonyms could help catch more locations. We have included a csv of demonyms in the github repository in case you would like to use this in the future.\n",
    "    * However, it is also important to note the problem of people mentioning a demonym when they actually live somewhere else (for example, we sometimes saw influencers write \"Irish in England\") \n",
    "        * Currently, to approach this problem, we have a feature in Filter 2 that notes whether or not a location is an ethnicity. \n",
    "        * Other ideas include: \n",
    "            * Doing more detailed analyses on location indicator words like \"in\" and \"living in\" -- oftentimes people would write phrases like \"French in Italy\" or \"Irish living in Spain.\" Even after adding our \"before context\" and \"after context\" analyses, some of these errors are still included in the final results. To improve this, one idea is to see what word precedes or follows the location indicator word. In the above examples, we have an ethnicity followed by a non-demonym country mention. Since the location indicator words imply that the second location is the home, we can be fairly confident that the first mention is not a home while the second one is. \n",
    "            * Thus, for demonyms, we can do more comparative analyses -- if other locations are mentioned in the text, we would want to see where these locations are mentioned and if they are preceded or followed by location indicator words. If the model identifies those other locations as more likely to be the home, then the probability of the demonym being a home should decrease.\n",
    "* More comparative approaches:\n",
    "    * This follows to a more general improvement -- rather than only looking at locations as individual location mentions, further steps should attempt to also look at all location mentions collectively for each influencer. This will be especially helpful for models and world travelers. Below is one example of a world traveler's profile description: \n",
    "\"📍: 🇳🇿\n",
    "✈️: London, Portugal, Spain and Luxembourg soon\n",
    "❤️:☀️🍾☕️ website: http://www.sarahseestheworld.com/;Passionate about travelling and returning home to Wellington. I also heart wifi and good coffee. Instagram @sarahseestworld\" \n",
    "        * To analyze this above description, we can look at all the location words comparatively (\"🇳🇿; London; Portugal; Spain; Luxembourg; Wellington\"), rather than independently. One way to do this is to tag each location based on three criteria:\n",
    "            * the location's type (demonym, emoji, country, city, etc.) \n",
    "            * the location's corresponding indicator words (pushpin emoji, airplane emoji, the word \"soon\", the word \"home\", etc.)\n",
    "            * the synonymous locations: in other words, the flag emoji (if applicable) and the broader location names that a person is also located in if they are located in a certain city, region, or state -- Wellington, for example, would be tagged with \"🇳🇿\" and \"New Zealand\"\n",
    "        * For the above example, the tags would be as follows: \n",
    "            *  🇳🇿 : type = emoji; indicators = 📍; synonyms = New Zealand\n",
    "            *  London: type = city; indicators = ✈️, \"soon\"; synonyms = UK, 🇬🇧\n",
    "            *  [similar tags for Portugal, Spain, Luxembourg]\n",
    "            *  Wellington: type = city; indicators = \"home\"; synonyms = New Zealand, 🇳🇿\n",
    "        * For Filter 2 specifically, a hierarchy could be established in which certain location indicators have higher probabilites than other words in indicating home locations. Two examples that show up here are that the pushpin emoji and the word \"home\" are often more likely to indicate homes than are the airplane emoji and the word \"soon.\" By establishing this hierarchy, home identification can become more accurate. Other weighting schemes can also be established for the other tags, like the location type category. Demonyms tend to be the least likely to be home locations, so these can be placed as lower likelihood under the other data types. Finally, the synonyms can be useful in matching different ways of writing the same location together. If a person uses 🇳🇿 and \"Wellington\" in one description, for example, these locations match (same synonyms) and could receive higher probability.\n",
    "* Context: \n",
    "    * Taking a more quantitative approach with regards to how many words to analyze before and after the location word can help with improving accuracy. Also, identifying whether or not other locations are mentioned in the context would be good. In this way, the problem of a location indicator appearing near a location word but not actually referencing that location word can be alleviated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
